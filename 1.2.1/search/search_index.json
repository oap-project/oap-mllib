{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OAP MLlib Overview OAP MLlib is an optimized package to accelerate machine learning algorithms in Apache Spark MLlib . It is compatible with Spark MLlib and leverages open source Intel\u00ae oneAPI Data Analytics Library (oneDAL) to provide highly optimized algorithms and get most out of CPU and GPU capabilities. It also take advantage of open source Intel\u00ae oneAPI Collective Communications Library (oneCCL) to provide efficient communication patterns in multi-node multi-GPU clusters. Compatibility OAP MLlib maintains the same API interfaces with Spark MLlib. That means the application built with Spark MLlib can be running directly with minimum configuration. Most of the algorithms can produce the same results that are identical with Spark MLlib. However due to the nature of distributed float point operations, there may be some small deviation from the original result, we will make sure the error is within acceptable range and the accuracy is on par with Spark MLlib. For those algorithms that are not accelerated by OAP MLlib, the original Spark MLlib one will be used. Getting Started Java/Scala Users Preferred Use a pre-built OAP MLlib JAR to get started. You can firstly download OAP package from OAP-JARs-Tarball and extract this Tarball to get oap-mllib-x.x.x.jar under oap-1.2.1-bin-spark-3.1.1/jars . Then you can refer to the following Running section to try out. Python/PySpark Users Preferred Use a pre-built JAR to get started. If you have finished OAP Installation Guide , you can find compiled OAP MLlib JAR oap-mllib-x.x.x.jar in $HOME/miniconda2/envs/oapenv/oap_jars/ . Then you can refer to the following Running section to try out. Building From Scratch You can also build the package from source code, please refer to Building section. Running Prerequisites CentOS 7.0+, Ubuntu 18.04 LTS+ Java JRE 8.0+ Runtime Apache Spark 3.0.0+ Generally, our common system requirements are the same with Intel\u00ae oneAPI Toolkit, please refer to here for details. Intel\u00ae oneAPI Toolkits components used by the project are already included into JAR package mentioned above. There are no extra installations for cluster nodes. Spark Configuration General Configuration YARN Cluster Manager Users usually run Spark application on YARN with client mode. In that case, you only need to add the following configurations in spark-defaults.conf or in spark-submit command line before running. # absolute path of the jar for uploading spark.files /path/to/oap-mllib-x.x.x.jar # absolute path of the jar for driver class path spark.driver.extraClassPath /path/to/oap-mllib-x.x.x.jar # relative path of the jar for executor class path spark.executor.extraClassPath ./oap-mllib-x.x.x.jar Standalone Cluster Manager For standalone cluster manager, you need to upload the jar to every node or use shared network folder and then specify absolute paths for extraClassPath. # absolute path of the jar for driver class path spark.driver.extraClassPath /path/to/oap-mllib-x.x.x.jar # absolute path of the jar for executor class path spark.executor.extraClassPath /path/to/oap-mllib-x.x.x.jar OAP MLlib Specific Configuration OAP MLlib adopted oneDAL as implementation backend. oneDAL requires enough native memory allocated for each executor. For large dataset, depending on algorithms, you may need to tune spark.executor.memoryOverhead to allocate enough native memory. Setting this value to larger than dataset size / executor number is a good starting point. OAP MLlib expects 1 executor acts as 1 oneCCL rank for compute. As spark.shuffle.reduceLocality.enabled option is true by default, when the dataset is not evenly distributed accross executors, this option may result in assigning more than 1 rank to single executor and task failing. The error could be fixed by setting spark.shuffle.reduceLocality.enabled to false . Sanity Check Setup env.sh $ cd conf $ cp env.sh.template env.sh Edit related variables in \" Minimun Settings \" of env.sh Upload example data files to HDFS $ cd examples $ hadoop fs -mkdir -p /user/$USER $ hadoop fs -copyFromLocal data $ hadoop fs -ls data Run K-means $ cd examples/kmeans $ ./build.sh $ ./run.sh PySpark Support As PySpark-based applications call their Scala couterparts, they shall be supported out-of-box. Examples can be found in the Examples section. Building Prerequisites We use Apache Maven to manage and build source code. The following tools and libraries are also needed to build OAP MLlib: JDK 8.0+ Apache Maven 3.6.2+ GNU GCC 4.8.5+ Intel\u00ae oneAPI Toolkits 2021.3.0 Components: DPC++/C++ Compiler (dpcpp/clang++) Data Analytics Library (oneDAL) Threading Building Blocks (oneTBB) Open Source Intel\u00ae oneAPI Collective Communications Library (oneCCL) Intel\u00ae oneAPI Toolkits and its components can be downloaded and install from here . Installation process for oneAPI using Package Managers (YUM (DNF), APT, and ZYPPER) is also available. Generally you only need to install oneAPI Base Toolkit for Linux with all or selected components mentioned above. Instead of using oneCCL included in Intel\u00ae oneAPI Toolkits, we prefer to build from open source oneCCL to resolve some bugs. More details about oneAPI can be found here . Scala and Java dependency descriptions are already included in Maven POM file. Note: You can refer to this script to install correct dependencies: DPC++/C++, oneDAL, oneTBB, oneCCL. Build Building oneCCL To clone and build from open source oneCCL, run the following commands: $ git clone https://github.com/oneapi-src/oneCCL $ cd oneCCL $ git checkout 2021.2.1 $ mkdir build && cd build $ cmake .. $ make -j install The generated files will be placed in /your/oneCCL_source_code/build/_install Building OAP MLlib To clone and checkout source code, run the following commands: $ git clone https://github.com/oap-project/oap-mllib.git Optional to checkout specific release branch: $ cd oap-mllib && git checkout ${version} We rely on environment variables to find required toolchains and libraries. Please make sure the following environment variables are set for building: Environment Description JAVA_HOME Path to JDK home directory DAALROOT Path to oneDAL home directory TBB_ROOT Path to oneTBB home directory CCL_ROOT Path to oneCCL home directory We suggest you to source setvars.sh script into current shell to setup building environments as following: $ source /opt/intel/oneapi/setvars.sh $ source /your/oneCCL_source_code/build/_install/env/setvars.sh Be noticed we are using our own built oneCCL instead, we should source oneCCL's setvars.sh to overwrite oneAPI one. You can also refer to this CI script to setup the building environments. If you prefer to buid your own open source oneDAL , oneTBB versions rather than use the ones included in oneAPI TookKits, you can refer to the related build instructions and manually source setvars.sh accordingly. To build, run the following commands: $ cd mllib-dal $ ./build.sh If no parameter is given, the Spark version 3.1.1 will be activated by default. You can also specify a different Spark version with option -p spark-x.x.x . For example: $ ./build.sh -p spark-3.0.0 The built JAR package will be placed in target directory with the name oap-mllib-x.x.x.jar . Examples Scala Examples Example Description kmeans K-means example for Scala pca PCA example for Scala als ALS example for Scala naive-bayes Naive Bayes example for Scala linear-regression Linear Regression example for Scala Python Examples Example Description kmeans-pyspark K-means example for PySpark pca-pyspark PCA example for PySpark als-pyspark ALS example for PySpark List of Accelerated Algorithms Algorithm Category Maturity K-Means CPU Stable K-Means GPU Experimental PCA CPU Stable PCA GPU Experimental ALS CPU Stable Naive Bayes CPU Experimental Linear Regression CPU Experimental","title":"User Guide"},{"location":"#oap-mllib","text":"","title":"OAP MLlib"},{"location":"#overview","text":"OAP MLlib is an optimized package to accelerate machine learning algorithms in Apache Spark MLlib . It is compatible with Spark MLlib and leverages open source Intel\u00ae oneAPI Data Analytics Library (oneDAL) to provide highly optimized algorithms and get most out of CPU and GPU capabilities. It also take advantage of open source Intel\u00ae oneAPI Collective Communications Library (oneCCL) to provide efficient communication patterns in multi-node multi-GPU clusters.","title":"Overview"},{"location":"#compatibility","text":"OAP MLlib maintains the same API interfaces with Spark MLlib. That means the application built with Spark MLlib can be running directly with minimum configuration. Most of the algorithms can produce the same results that are identical with Spark MLlib. However due to the nature of distributed float point operations, there may be some small deviation from the original result, we will make sure the error is within acceptable range and the accuracy is on par with Spark MLlib. For those algorithms that are not accelerated by OAP MLlib, the original Spark MLlib one will be used.","title":"Compatibility"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#javascala-users-preferred","text":"Use a pre-built OAP MLlib JAR to get started. You can firstly download OAP package from OAP-JARs-Tarball and extract this Tarball to get oap-mllib-x.x.x.jar under oap-1.2.1-bin-spark-3.1.1/jars . Then you can refer to the following Running section to try out.","title":"Java/Scala Users Preferred"},{"location":"#pythonpyspark-users-preferred","text":"Use a pre-built JAR to get started. If you have finished OAP Installation Guide , you can find compiled OAP MLlib JAR oap-mllib-x.x.x.jar in $HOME/miniconda2/envs/oapenv/oap_jars/ . Then you can refer to the following Running section to try out.","title":"Python/PySpark Users Preferred"},{"location":"#building-from-scratch","text":"You can also build the package from source code, please refer to Building section.","title":"Building From Scratch"},{"location":"#running","text":"","title":"Running"},{"location":"#prerequisites","text":"CentOS 7.0+, Ubuntu 18.04 LTS+ Java JRE 8.0+ Runtime Apache Spark 3.0.0+ Generally, our common system requirements are the same with Intel\u00ae oneAPI Toolkit, please refer to here for details. Intel\u00ae oneAPI Toolkits components used by the project are already included into JAR package mentioned above. There are no extra installations for cluster nodes.","title":"Prerequisites"},{"location":"#spark-configuration","text":"","title":"Spark Configuration"},{"location":"#general-configuration","text":"","title":"General Configuration"},{"location":"#yarn-cluster-manager","text":"Users usually run Spark application on YARN with client mode. In that case, you only need to add the following configurations in spark-defaults.conf or in spark-submit command line before running. # absolute path of the jar for uploading spark.files /path/to/oap-mllib-x.x.x.jar # absolute path of the jar for driver class path spark.driver.extraClassPath /path/to/oap-mllib-x.x.x.jar # relative path of the jar for executor class path spark.executor.extraClassPath ./oap-mllib-x.x.x.jar","title":"YARN Cluster Manager"},{"location":"#standalone-cluster-manager","text":"For standalone cluster manager, you need to upload the jar to every node or use shared network folder and then specify absolute paths for extraClassPath. # absolute path of the jar for driver class path spark.driver.extraClassPath /path/to/oap-mllib-x.x.x.jar # absolute path of the jar for executor class path spark.executor.extraClassPath /path/to/oap-mllib-x.x.x.jar","title":"Standalone Cluster Manager"},{"location":"#oap-mllib-specific-configuration","text":"OAP MLlib adopted oneDAL as implementation backend. oneDAL requires enough native memory allocated for each executor. For large dataset, depending on algorithms, you may need to tune spark.executor.memoryOverhead to allocate enough native memory. Setting this value to larger than dataset size / executor number is a good starting point. OAP MLlib expects 1 executor acts as 1 oneCCL rank for compute. As spark.shuffle.reduceLocality.enabled option is true by default, when the dataset is not evenly distributed accross executors, this option may result in assigning more than 1 rank to single executor and task failing. The error could be fixed by setting spark.shuffle.reduceLocality.enabled to false .","title":"OAP MLlib Specific Configuration"},{"location":"#sanity-check","text":"","title":"Sanity Check"},{"location":"#setup-envsh","text":"$ cd conf $ cp env.sh.template env.sh Edit related variables in \" Minimun Settings \" of env.sh","title":"Setup env.sh"},{"location":"#upload-example-data-files-to-hdfs","text":"$ cd examples $ hadoop fs -mkdir -p /user/$USER $ hadoop fs -copyFromLocal data $ hadoop fs -ls data","title":"Upload example data files to HDFS"},{"location":"#run-k-means","text":"$ cd examples/kmeans $ ./build.sh $ ./run.sh","title":"Run K-means"},{"location":"#pyspark-support","text":"As PySpark-based applications call their Scala couterparts, they shall be supported out-of-box. Examples can be found in the Examples section.","title":"PySpark Support"},{"location":"#building","text":"","title":"Building"},{"location":"#prerequisites_1","text":"We use Apache Maven to manage and build source code. The following tools and libraries are also needed to build OAP MLlib: JDK 8.0+ Apache Maven 3.6.2+ GNU GCC 4.8.5+ Intel\u00ae oneAPI Toolkits 2021.3.0 Components: DPC++/C++ Compiler (dpcpp/clang++) Data Analytics Library (oneDAL) Threading Building Blocks (oneTBB) Open Source Intel\u00ae oneAPI Collective Communications Library (oneCCL) Intel\u00ae oneAPI Toolkits and its components can be downloaded and install from here . Installation process for oneAPI using Package Managers (YUM (DNF), APT, and ZYPPER) is also available. Generally you only need to install oneAPI Base Toolkit for Linux with all or selected components mentioned above. Instead of using oneCCL included in Intel\u00ae oneAPI Toolkits, we prefer to build from open source oneCCL to resolve some bugs. More details about oneAPI can be found here . Scala and Java dependency descriptions are already included in Maven POM file. Note: You can refer to this script to install correct dependencies: DPC++/C++, oneDAL, oneTBB, oneCCL.","title":"Prerequisites"},{"location":"#build","text":"","title":"Build"},{"location":"#building-oneccl","text":"To clone and build from open source oneCCL, run the following commands: $ git clone https://github.com/oneapi-src/oneCCL $ cd oneCCL $ git checkout 2021.2.1 $ mkdir build && cd build $ cmake .. $ make -j install The generated files will be placed in /your/oneCCL_source_code/build/_install","title":"Building oneCCL"},{"location":"#building-oap-mllib","text":"To clone and checkout source code, run the following commands: $ git clone https://github.com/oap-project/oap-mllib.git Optional to checkout specific release branch: $ cd oap-mllib && git checkout ${version} We rely on environment variables to find required toolchains and libraries. Please make sure the following environment variables are set for building: Environment Description JAVA_HOME Path to JDK home directory DAALROOT Path to oneDAL home directory TBB_ROOT Path to oneTBB home directory CCL_ROOT Path to oneCCL home directory We suggest you to source setvars.sh script into current shell to setup building environments as following: $ source /opt/intel/oneapi/setvars.sh $ source /your/oneCCL_source_code/build/_install/env/setvars.sh Be noticed we are using our own built oneCCL instead, we should source oneCCL's setvars.sh to overwrite oneAPI one. You can also refer to this CI script to setup the building environments. If you prefer to buid your own open source oneDAL , oneTBB versions rather than use the ones included in oneAPI TookKits, you can refer to the related build instructions and manually source setvars.sh accordingly. To build, run the following commands: $ cd mllib-dal $ ./build.sh If no parameter is given, the Spark version 3.1.1 will be activated by default. You can also specify a different Spark version with option -p spark-x.x.x . For example: $ ./build.sh -p spark-3.0.0 The built JAR package will be placed in target directory with the name oap-mllib-x.x.x.jar .","title":"Building OAP MLlib"},{"location":"#examples","text":"","title":"Examples"},{"location":"#scala-examples","text":"Example Description kmeans K-means example for Scala pca PCA example for Scala als ALS example for Scala naive-bayes Naive Bayes example for Scala linear-regression Linear Regression example for Scala","title":"Scala Examples"},{"location":"#python-examples","text":"Example Description kmeans-pyspark K-means example for PySpark pca-pyspark PCA example for PySpark als-pyspark ALS example for PySpark","title":"Python Examples"},{"location":"#list-of-accelerated-algorithms","text":"Algorithm Category Maturity K-Means CPU Stable K-Means GPU Experimental PCA CPU Stable PCA GPU Experimental ALS CPU Stable Naive Bayes CPU Experimental Linear Regression CPU Experimental","title":"List of Accelerated Algorithms"},{"location":"OAP-Developer-Guide/","text":"OAP Developer Guide This document contains the instructions & scripts on installing necessary dependencies and building OAP modules. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Gazelle Plugin Building OAP Prerequisites We provide scripts to help automatically install dependencies required, please change to root user and run: # git clone -b <tag-version> https://github.com/oap-project/oap-tools.git # cd oap-tools # sh dev/install-compile-time-dependencies.sh Note : oap-tools tag version v1.2.1 corresponds to all OAP modules' tag version v1.2.1 . Then the dependencies below will be installed: Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow LLVM Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. Building Building OAP OAP is built with Apache Maven and Oracle Java 8. To build OAP package, run command below then you can find a tarball named oap-$VERSION-*.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package , which contains all OAP module jars. Change to root user, run # cd oap-tools # sh dev/compile-oap.sh Building OAP specific module If you just want to build a specific OAP Module, such as sql-ds-cache , change to root user, then run: # cd oap-tools # sh dev/compile-oap.sh --component=sql-ds-cache","title":"OAP Developer Guide"},{"location":"OAP-Developer-Guide/#oap-developer-guide","text":"This document contains the instructions & scripts on installing necessary dependencies and building OAP modules. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Gazelle Plugin","title":"OAP Developer Guide"},{"location":"OAP-Developer-Guide/#building-oap","text":"","title":"Building OAP"},{"location":"OAP-Developer-Guide/#prerequisites","text":"We provide scripts to help automatically install dependencies required, please change to root user and run: # git clone -b <tag-version> https://github.com/oap-project/oap-tools.git # cd oap-tools # sh dev/install-compile-time-dependencies.sh Note : oap-tools tag version v1.2.1 corresponds to all OAP modules' tag version v1.2.1 . Then the dependencies below will be installed: Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow LLVM Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance.","title":"Prerequisites"},{"location":"OAP-Developer-Guide/#building","text":"","title":"Building"},{"location":"OAP-Developer-Guide/#building-oap_1","text":"OAP is built with Apache Maven and Oracle Java 8. To build OAP package, run command below then you can find a tarball named oap-$VERSION-*.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package , which contains all OAP module jars. Change to root user, run # cd oap-tools # sh dev/compile-oap.sh","title":"Building OAP"},{"location":"OAP-Developer-Guide/#building-oap-specific-module","text":"If you just want to build a specific OAP Module, such as sql-ds-cache , change to root user, then run: # cd oap-tools # sh dev/compile-oap.sh --component=sql-ds-cache","title":"Building OAP specific module"},{"location":"OAP-Installation-Guide/","text":"OAP Installation Guide This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine. Contents Prerequisites Installing OAP Configuration Prerequisites OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly. Installing OAP Create a Conda environment and install OAP Conda package. $ conda create -n oapenv -c conda-forge -c intel -y oap=1.2.1 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Extra Steps for Shuffle Remote PMem Extension If you use one of OAP features -- PMem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details. Configuration Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar Then you can follow the corresponding feature documents for more details to use them.","title":"OAP Installation Guide"},{"location":"OAP-Installation-Guide/#oap-installation-guide","text":"This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine.","title":"OAP Installation Guide"},{"location":"OAP-Installation-Guide/#contents","text":"Prerequisites Installing OAP Configuration","title":"Contents"},{"location":"OAP-Installation-Guide/#prerequisites","text":"OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly.","title":"Prerequisites"},{"location":"OAP-Installation-Guide/#installing-oap","text":"Create a Conda environment and install OAP Conda package. $ conda create -n oapenv -c conda-forge -c intel -y oap=1.2.1 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI","title":"Installing OAP"},{"location":"OAP-Installation-Guide/#extra-steps-for-shuffle-remote-pmem-extension","text":"If you use one of OAP features -- PMem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details.","title":"Extra Steps for Shuffle Remote PMem Extension"},{"location":"OAP-Installation-Guide/#configuration","text":"Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar Then you can follow the corresponding feature documents for more details to use them.","title":"Configuration"},{"location":"User-Guide/","text":"OAP MLlib Overview OAP MLlib is an optimized package to accelerate machine learning algorithms in Apache Spark MLlib . It is compatible with Spark MLlib and leverages open source Intel\u00ae oneAPI Data Analytics Library (oneDAL) to provide highly optimized algorithms and get most out of CPU and GPU capabilities. It also take advantage of open source Intel\u00ae oneAPI Collective Communications Library (oneCCL) to provide efficient communication patterns in multi-node multi-GPU clusters. Compatibility OAP MLlib maintains the same API interfaces with Spark MLlib. That means the application built with Spark MLlib can be running directly with minimum configuration. Most of the algorithms can produce the same results that are identical with Spark MLlib. However due to the nature of distributed float point operations, there may be some small deviation from the original result, we will make sure the error is within acceptable range and the accuracy is on par with Spark MLlib. For those algorithms that are not accelerated by OAP MLlib, the original Spark MLlib one will be used. Getting Started Java/Scala Users Preferred Use a pre-built OAP MLlib JAR to get started. You can firstly download OAP package from OAP-JARs-Tarball and extract this Tarball to get oap-mllib-x.x.x.jar under oap-1.2.1-bin-spark-3.1.1/jars . Then you can refer to the following Running section to try out. Python/PySpark Users Preferred Use a pre-built JAR to get started. If you have finished OAP Installation Guide , you can find compiled OAP MLlib JAR oap-mllib-x.x.x.jar in $HOME/miniconda2/envs/oapenv/oap_jars/ . Then you can refer to the following Running section to try out. Building From Scratch You can also build the package from source code, please refer to Building section. Running Prerequisites CentOS 7.0+, Ubuntu 18.04 LTS+ Java JRE 8.0+ Runtime Apache Spark 3.0.0+ Generally, our common system requirements are the same with Intel\u00ae oneAPI Toolkit, please refer to here for details. Intel\u00ae oneAPI Toolkits components used by the project are already included into JAR package mentioned above. There are no extra installations for cluster nodes. Spark Configuration General Configuration YARN Cluster Manager Users usually run Spark application on YARN with client mode. In that case, you only need to add the following configurations in spark-defaults.conf or in spark-submit command line before running. # absolute path of the jar for uploading spark.files /path/to/oap-mllib-x.x.x.jar # absolute path of the jar for driver class path spark.driver.extraClassPath /path/to/oap-mllib-x.x.x.jar # relative path of the jar for executor class path spark.executor.extraClassPath ./oap-mllib-x.x.x.jar Standalone Cluster Manager For standalone cluster manager, you need to upload the jar to every node or use shared network folder and then specify absolute paths for extraClassPath. # absolute path of the jar for driver class path spark.driver.extraClassPath /path/to/oap-mllib-x.x.x.jar # absolute path of the jar for executor class path spark.executor.extraClassPath /path/to/oap-mllib-x.x.x.jar OAP MLlib Specific Configuration OAP MLlib adopted oneDAL as implementation backend. oneDAL requires enough native memory allocated for each executor. For large dataset, depending on algorithms, you may need to tune spark.executor.memoryOverhead to allocate enough native memory. Setting this value to larger than dataset size / executor number is a good starting point. OAP MLlib expects 1 executor acts as 1 oneCCL rank for compute. As spark.shuffle.reduceLocality.enabled option is true by default, when the dataset is not evenly distributed accross executors, this option may result in assigning more than 1 rank to single executor and task failing. The error could be fixed by setting spark.shuffle.reduceLocality.enabled to false . Sanity Check Setup env.sh $ cd conf $ cp env.sh.template env.sh Edit related variables in \" Minimun Settings \" of env.sh Upload example data files to HDFS $ cd examples $ hadoop fs -mkdir -p /user/$USER $ hadoop fs -copyFromLocal data $ hadoop fs -ls data Run K-means $ cd examples/kmeans $ ./build.sh $ ./run.sh PySpark Support As PySpark-based applications call their Scala couterparts, they shall be supported out-of-box. Examples can be found in the Examples section. Building Prerequisites We use Apache Maven to manage and build source code. The following tools and libraries are also needed to build OAP MLlib: JDK 8.0+ Apache Maven 3.6.2+ GNU GCC 4.8.5+ Intel\u00ae oneAPI Toolkits 2021.3.0 Components: DPC++/C++ Compiler (dpcpp/clang++) Data Analytics Library (oneDAL) Threading Building Blocks (oneTBB) Open Source Intel\u00ae oneAPI Collective Communications Library (oneCCL) Intel\u00ae oneAPI Toolkits and its components can be downloaded and install from here . Installation process for oneAPI using Package Managers (YUM (DNF), APT, and ZYPPER) is also available. Generally you only need to install oneAPI Base Toolkit for Linux with all or selected components mentioned above. Instead of using oneCCL included in Intel\u00ae oneAPI Toolkits, we prefer to build from open source oneCCL to resolve some bugs. More details about oneAPI can be found here . Scala and Java dependency descriptions are already included in Maven POM file. Note: You can refer to this script to install correct dependencies: DPC++/C++, oneDAL, oneTBB, oneCCL. Build Building oneCCL To clone and build from open source oneCCL, run the following commands: $ git clone https://github.com/oneapi-src/oneCCL $ cd oneCCL $ git checkout 2021.2.1 $ mkdir build && cd build $ cmake .. $ make -j install The generated files will be placed in /your/oneCCL_source_code/build/_install Building OAP MLlib To clone and checkout source code, run the following commands: $ git clone https://github.com/oap-project/oap-mllib.git Optional to checkout specific release branch: $ cd oap-mllib && git checkout ${version} We rely on environment variables to find required toolchains and libraries. Please make sure the following environment variables are set for building: Environment Description JAVA_HOME Path to JDK home directory DAALROOT Path to oneDAL home directory TBB_ROOT Path to oneTBB home directory CCL_ROOT Path to oneCCL home directory We suggest you to source setvars.sh script into current shell to setup building environments as following: $ source /opt/intel/oneapi/setvars.sh $ source /your/oneCCL_source_code/build/_install/env/setvars.sh Be noticed we are using our own built oneCCL instead, we should source oneCCL's setvars.sh to overwrite oneAPI one. You can also refer to this CI script to setup the building environments. If you prefer to buid your own open source oneDAL , oneTBB versions rather than use the ones included in oneAPI TookKits, you can refer to the related build instructions and manually source setvars.sh accordingly. To build, run the following commands: $ cd mllib-dal $ ./build.sh If no parameter is given, the Spark version 3.1.1 will be activated by default. You can also specify a different Spark version with option -p spark-x.x.x . For example: $ ./build.sh -p spark-3.0.0 The built JAR package will be placed in target directory with the name oap-mllib-x.x.x.jar . Examples Scala Examples Example Description kmeans K-means example for Scala pca PCA example for Scala als ALS example for Scala naive-bayes Naive Bayes example for Scala linear-regression Linear Regression example for Scala Python Examples Example Description kmeans-pyspark K-means example for PySpark pca-pyspark PCA example for PySpark als-pyspark ALS example for PySpark List of Accelerated Algorithms Algorithm Category Maturity K-Means CPU Stable K-Means GPU Experimental PCA CPU Stable PCA GPU Experimental ALS CPU Stable Naive Bayes CPU Experimental Linear Regression CPU Experimental","title":"OAP MLlib"},{"location":"User-Guide/#oap-mllib","text":"","title":"OAP MLlib"},{"location":"User-Guide/#overview","text":"OAP MLlib is an optimized package to accelerate machine learning algorithms in Apache Spark MLlib . It is compatible with Spark MLlib and leverages open source Intel\u00ae oneAPI Data Analytics Library (oneDAL) to provide highly optimized algorithms and get most out of CPU and GPU capabilities. It also take advantage of open source Intel\u00ae oneAPI Collective Communications Library (oneCCL) to provide efficient communication patterns in multi-node multi-GPU clusters.","title":"Overview"},{"location":"User-Guide/#compatibility","text":"OAP MLlib maintains the same API interfaces with Spark MLlib. That means the application built with Spark MLlib can be running directly with minimum configuration. Most of the algorithms can produce the same results that are identical with Spark MLlib. However due to the nature of distributed float point operations, there may be some small deviation from the original result, we will make sure the error is within acceptable range and the accuracy is on par with Spark MLlib. For those algorithms that are not accelerated by OAP MLlib, the original Spark MLlib one will be used.","title":"Compatibility"},{"location":"User-Guide/#getting-started","text":"","title":"Getting Started"},{"location":"User-Guide/#javascala-users-preferred","text":"Use a pre-built OAP MLlib JAR to get started. You can firstly download OAP package from OAP-JARs-Tarball and extract this Tarball to get oap-mllib-x.x.x.jar under oap-1.2.1-bin-spark-3.1.1/jars . Then you can refer to the following Running section to try out.","title":"Java/Scala Users Preferred"},{"location":"User-Guide/#pythonpyspark-users-preferred","text":"Use a pre-built JAR to get started. If you have finished OAP Installation Guide , you can find compiled OAP MLlib JAR oap-mllib-x.x.x.jar in $HOME/miniconda2/envs/oapenv/oap_jars/ . Then you can refer to the following Running section to try out.","title":"Python/PySpark Users Preferred"},{"location":"User-Guide/#building-from-scratch","text":"You can also build the package from source code, please refer to Building section.","title":"Building From Scratch"},{"location":"User-Guide/#running","text":"","title":"Running"},{"location":"User-Guide/#prerequisites","text":"CentOS 7.0+, Ubuntu 18.04 LTS+ Java JRE 8.0+ Runtime Apache Spark 3.0.0+ Generally, our common system requirements are the same with Intel\u00ae oneAPI Toolkit, please refer to here for details. Intel\u00ae oneAPI Toolkits components used by the project are already included into JAR package mentioned above. There are no extra installations for cluster nodes.","title":"Prerequisites"},{"location":"User-Guide/#spark-configuration","text":"","title":"Spark Configuration"},{"location":"User-Guide/#general-configuration","text":"","title":"General Configuration"},{"location":"User-Guide/#yarn-cluster-manager","text":"Users usually run Spark application on YARN with client mode. In that case, you only need to add the following configurations in spark-defaults.conf or in spark-submit command line before running. # absolute path of the jar for uploading spark.files /path/to/oap-mllib-x.x.x.jar # absolute path of the jar for driver class path spark.driver.extraClassPath /path/to/oap-mllib-x.x.x.jar # relative path of the jar for executor class path spark.executor.extraClassPath ./oap-mllib-x.x.x.jar","title":"YARN Cluster Manager"},{"location":"User-Guide/#standalone-cluster-manager","text":"For standalone cluster manager, you need to upload the jar to every node or use shared network folder and then specify absolute paths for extraClassPath. # absolute path of the jar for driver class path spark.driver.extraClassPath /path/to/oap-mllib-x.x.x.jar # absolute path of the jar for executor class path spark.executor.extraClassPath /path/to/oap-mllib-x.x.x.jar","title":"Standalone Cluster Manager"},{"location":"User-Guide/#oap-mllib-specific-configuration","text":"OAP MLlib adopted oneDAL as implementation backend. oneDAL requires enough native memory allocated for each executor. For large dataset, depending on algorithms, you may need to tune spark.executor.memoryOverhead to allocate enough native memory. Setting this value to larger than dataset size / executor number is a good starting point. OAP MLlib expects 1 executor acts as 1 oneCCL rank for compute. As spark.shuffle.reduceLocality.enabled option is true by default, when the dataset is not evenly distributed accross executors, this option may result in assigning more than 1 rank to single executor and task failing. The error could be fixed by setting spark.shuffle.reduceLocality.enabled to false .","title":"OAP MLlib Specific Configuration"},{"location":"User-Guide/#sanity-check","text":"","title":"Sanity Check"},{"location":"User-Guide/#setup-envsh","text":"$ cd conf $ cp env.sh.template env.sh Edit related variables in \" Minimun Settings \" of env.sh","title":"Setup env.sh"},{"location":"User-Guide/#upload-example-data-files-to-hdfs","text":"$ cd examples $ hadoop fs -mkdir -p /user/$USER $ hadoop fs -copyFromLocal data $ hadoop fs -ls data","title":"Upload example data files to HDFS"},{"location":"User-Guide/#run-k-means","text":"$ cd examples/kmeans $ ./build.sh $ ./run.sh","title":"Run K-means"},{"location":"User-Guide/#pyspark-support","text":"As PySpark-based applications call their Scala couterparts, they shall be supported out-of-box. Examples can be found in the Examples section.","title":"PySpark Support"},{"location":"User-Guide/#building","text":"","title":"Building"},{"location":"User-Guide/#prerequisites_1","text":"We use Apache Maven to manage and build source code. The following tools and libraries are also needed to build OAP MLlib: JDK 8.0+ Apache Maven 3.6.2+ GNU GCC 4.8.5+ Intel\u00ae oneAPI Toolkits 2021.3.0 Components: DPC++/C++ Compiler (dpcpp/clang++) Data Analytics Library (oneDAL) Threading Building Blocks (oneTBB) Open Source Intel\u00ae oneAPI Collective Communications Library (oneCCL) Intel\u00ae oneAPI Toolkits and its components can be downloaded and install from here . Installation process for oneAPI using Package Managers (YUM (DNF), APT, and ZYPPER) is also available. Generally you only need to install oneAPI Base Toolkit for Linux with all or selected components mentioned above. Instead of using oneCCL included in Intel\u00ae oneAPI Toolkits, we prefer to build from open source oneCCL to resolve some bugs. More details about oneAPI can be found here . Scala and Java dependency descriptions are already included in Maven POM file. Note: You can refer to this script to install correct dependencies: DPC++/C++, oneDAL, oneTBB, oneCCL.","title":"Prerequisites"},{"location":"User-Guide/#build","text":"","title":"Build"},{"location":"User-Guide/#building-oneccl","text":"To clone and build from open source oneCCL, run the following commands: $ git clone https://github.com/oneapi-src/oneCCL $ cd oneCCL $ git checkout 2021.2.1 $ mkdir build && cd build $ cmake .. $ make -j install The generated files will be placed in /your/oneCCL_source_code/build/_install","title":"Building oneCCL"},{"location":"User-Guide/#building-oap-mllib","text":"To clone and checkout source code, run the following commands: $ git clone https://github.com/oap-project/oap-mllib.git Optional to checkout specific release branch: $ cd oap-mllib && git checkout ${version} We rely on environment variables to find required toolchains and libraries. Please make sure the following environment variables are set for building: Environment Description JAVA_HOME Path to JDK home directory DAALROOT Path to oneDAL home directory TBB_ROOT Path to oneTBB home directory CCL_ROOT Path to oneCCL home directory We suggest you to source setvars.sh script into current shell to setup building environments as following: $ source /opt/intel/oneapi/setvars.sh $ source /your/oneCCL_source_code/build/_install/env/setvars.sh Be noticed we are using our own built oneCCL instead, we should source oneCCL's setvars.sh to overwrite oneAPI one. You can also refer to this CI script to setup the building environments. If you prefer to buid your own open source oneDAL , oneTBB versions rather than use the ones included in oneAPI TookKits, you can refer to the related build instructions and manually source setvars.sh accordingly. To build, run the following commands: $ cd mllib-dal $ ./build.sh If no parameter is given, the Spark version 3.1.1 will be activated by default. You can also specify a different Spark version with option -p spark-x.x.x . For example: $ ./build.sh -p spark-3.0.0 The built JAR package will be placed in target directory with the name oap-mllib-x.x.x.jar .","title":"Building OAP MLlib"},{"location":"User-Guide/#examples","text":"","title":"Examples"},{"location":"User-Guide/#scala-examples","text":"Example Description kmeans K-means example for Scala pca PCA example for Scala als ALS example for Scala naive-bayes Naive Bayes example for Scala linear-regression Linear Regression example for Scala","title":"Scala Examples"},{"location":"User-Guide/#python-examples","text":"Example Description kmeans-pyspark K-means example for PySpark pca-pyspark PCA example for PySpark als-pyspark ALS example for PySpark","title":"Python Examples"},{"location":"User-Guide/#list-of-accelerated-algorithms","text":"Algorithm Category Maturity K-Means CPU Stable K-Means GPU Experimental PCA CPU Stable PCA GPU Experimental ALS CPU Stable Naive Bayes CPU Experimental Linear Regression CPU Experimental","title":"List of Accelerated Algorithms"}]}